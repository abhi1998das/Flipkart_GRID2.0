# -*- coding: utf-8 -*-
"""Flipkart_api.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XXXWIaLRJNIF8kF8q8HojGS7fX9uRZ4o
"""





import numpy as np
import cv2 as cv
def extract_features(image_path, vector_size=32):
    #image = imread(image_path, mode="RGB")
    image = img = cv.imread(image_path)
    #print(image)
    try:
        alg = cv.xfeatures2d.SIFT_create()
        kps = alg.detect(image,None)
        kps = sorted(kps, key=lambda x: -x.response)[:vector_size]
        kps, dsc = alg.compute(image, kps)
        dsc = dsc.flatten()
        needed_size = (vector_size * 64)
        if dsc.size < needed_size:
            dsc = np.concatenate([dsc, np.zeros(needed_size - dsc.size)])
    except cv.error as e:
        print('Error: ', e)
        return None

    return dsc

countt=8
map_main={}

import os, sys
from annoy import AnnoyIndex
import urllib.request
map={}
# Open a file
path = "/content/ALL"
dirs = os.listdir( path )
f = 4096
t = AnnoyIndex(f, 'euclidean')
# This would print all the files and directories
i=0
for file in dirs:
   if(file.split('.')[1]=="jpg"):
     features=extract_features(path+"/"+file)
     t.add_item(i, features)
     map[i]=file
     map_main[file]=file.split('.')[0].split('_')[0][-1]
     i=i+1

t.build(10) # 10 trees
t.save('Flipkart.ann')

mp={}
mp[1]=1
mp[2]=1
mp[3]=1
mp[4]=1
mp[5]=2
mp[6]=1
mp[7]=2
mp[8]=1
mp[9]=1
mp[10]=1

import gdown

def trained(countt):
  map={}
  # Open a file
  path = "/content/ALL"
  dirs = os.listdir( path )
  f = 4096
  t = AnnoyIndex(f, 'euclidean')
  # This would print all the files and directories
  i=0
  for file in dirs:
    if(file.split('.')[1]=="jpg"):
      features=extract_features(path+"/"+file)
      t.add_item(i, features)
      map[i]=file
      map_main[file]=file.split('.')[0].split('_')[0][-1]
      i=i+1
  path="/content/temp/ALL"
  dirs = os.listdir( path )
  for file in dirs:
    if(file.split('.')[1]=="jpg"):
      features=extract_features(path+"/"+file)
      t.add_item(i, features)
      map[i]=file
      map_main[file]=countt
      i=i+1
  t.build(10) # 10 trees
  os.remove("Flipkart.ann")
  t.save('Flipkart.ann')

def api1(path):

  url = ''

  data = {'file': open(path, 'rb')}

  response = requests.post(url, auth=requests.auth.HTTPBasicAuth('', ''), files=data)

  return response

def api2(path):
  url = ''

  data = {'file': open(path, 'rb')}

  response = requests.post(url, auth=requests.auth.HTTPBasicAuth('', ''), files=data)

  return response

def classify_and_analyze(path):
  filename = 'a1.jpg'
  image_url = path
  urllib.request.urlretrieve(image_url, filename)
  # calling urlretrieve function to get resource
  features=extract_features('a1.jpg')
  
  u = AnnoyIndex(4096, 'euclidean')
  u.load('') # super fast, will just mmap the file
  ans1=map_main[map[u.get_nns_by_vector(features, 1)[0]]]
  typee=mp[int(ans1)]
  #print(typee)
  if typee==1:
    req=api1('a1.jpg')
  else:
    req=api2('a1.jpg')
  os.remove("a1.jpg")
  return ans1,req

import requests

def train_siftt(link,countt):
  total="https://drive.google.com/uc?id="+link
  output="files.zip"
  gdown.download(total,output,quiet=False)
  import zipfile
  with zipfile.ZipFile("files.zip","r") as zip_ref:
    zip_ref.extractall("")
  trained(countt)

def extract_tab(link):
  import json
  from ExtractTable import ExtractTable
  et_sess = ExtractTable(api_key='')        # Replace your VALID API Key here
  #print(et_sess.check_usage())  
  import pandas as pd
  table_data = et_sess.process_file(filepath=link, output_format="df")
  i=0
  d={}
  for table in table_data:
    d[i]= table.to_json()
    i=i+1
  return d

from flask_ngrok import run_with_ngrok
from flask import Flask,jsonify
from flask import request
app = Flask(__name__)
run_with_ngrok(app)   #starts ngrok when the app is run
@app.route('/', methods=['POST'])
def create_task():
    #print(request.form)
    ans,req=classify_and_analyze(request.form['url'])
    print(req.text)
    task = {
        'type': ans,
        'rest':req.text
    }
    return jsonify({'task': task}), 201
@app.route('/train', methods=['POST'])
def create_task2():
    #print(request.form)
    #ans,req=classify_and_analyze(request.form['url'])
    #cnt=global countt
    ans=train_siftt(request.form['url'],8)
    #cnt=cnt+1
    #global countt=cnt
    task = {
        'remark':"Training successfull",
        'Number of templates':8
    }
    return jsonify({'task': task}), 201
@app.route('/table', methods=['POST'])
def create_task3():
    #print(request.form)
    #ans,req=classify_and_analyze(request.form['url'])
    ans=extract_tab(request.form['url'])
    
    task = {
        'remark':"Table extracted successfully",
        'Tables':ans
    }
    return jsonify({'task': task}), 201
app.run()





